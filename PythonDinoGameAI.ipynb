{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally based off of the 'Build a Chrome Dino Game AI Model with Python' tutorial by Nicholas Renotte\n",
    "# Link: https://www.youtube.com/watch?v=vahwuupy81A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Dependencies\n",
    "# To avoid the zsh not found error, use backslashes before special characters such as '?', '[', '=' and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bef15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement learning framework, cousin to sklearn\n",
    "#!pip3 install stable-baselines3[extra] protobuf==3.20.* # Regular Installation\n",
    "#!pip3 install stable-baselines3\\[extra\\] protobuf\\=\\=3.20.\\* # ZSH Installation\n",
    "\n",
    "#!pip3 install gym\n",
    "#!pip3 install mss\n",
    "#!pip install opencv-python\n",
    "\n",
    "#!pip3 install pydirectinput # For Windows\n",
    "#!pip3 install pyautogui # Works fine on macOS directly & browser based games like 'Dino Game'\n",
    "\n",
    "#!pip3 install pynput\n",
    "#!pip3 install python-libxdo\n",
    "\n",
    "# Multiple Dependencies including Python 3.6+, Python Imaging Library or Pillow and Google Tesseract OCR\n",
    "# Google Tesseract OCR has binaries for Ubuntu & Windows, for Mac and ARM M1 chips, compilling needed\n",
    "#!pip3 install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfe1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "\n",
    "# Enables in-game captured frame processing\n",
    "import cv2\n",
    "\n",
    "# Transformational framework\n",
    "import numpy as np\n",
    "\n",
    "# File path managements\n",
    "import os\n",
    "\n",
    "# For sending in commands\n",
    "#import pydirectinput # For Windows\n",
    "\n",
    "# On macOS, will request accessibility access from terminal\n",
    "# Security Preferences -> Security & Privacy -> Privacy -> Accessibility -> Allow Terminal\n",
    "import pyautogui\n",
    "\n",
    "# For extracting the game over text component via OCR\n",
    "import pytesseract\n",
    "\n",
    "# For pausing\n",
    "import time\n",
    "\n",
    "# Environment components\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "# For visualizing captured in game screen frames\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For screen capturing, faster than opencv\n",
    "from mss import mss\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# For checking whether a valid environment is up and running whilst training the AI model\n",
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "# For saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Many video games utilize Direct X, which essentially bypasses the OS and will not register virtual key press\n",
    "# inputs, instead requiring direct key press inputs, meaning PyAutoGui might not work inside the game window\n",
    "#from pynput.keyboard import Key\n",
    "#from pynput.keyboard import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building A Custom Game Environment\n",
    "class WebGame(Env):\n",
    "    \n",
    "    # Sets up the environment action and observation shapes\n",
    "    def __init__(self):\n",
    "        # Subclasses the model\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sets up the observation spaces\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = (1, 83, 100), dtype = np.uint8)\n",
    "        \n",
    "        # 3 Different Possible Actions: Jump, Duck & No Action\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        # Defines the extraction parameters for the game\n",
    "        self.screen_capture = mss()\n",
    "        self.game_location = {'top': 300, 'left': 0, 'width': 600, 'height': 500}\n",
    "        self.game_over_location = {'top': 405, 'left': 630, 'width': 660, 'height': 70}\n",
    "    \n",
    "    # Carries out an action and actually does something within the game\n",
    "    def step(self, action):\n",
    "        # Action Keys:\n",
    "        # 0 - Spacebar (Jump)\n",
    "        # 1 - Down (Duck)\n",
    "        # 2 - No Action\n",
    "        action_map = {\n",
    "            0: 'space',\n",
    "            1: 'down',\n",
    "            2: 'no_operation'\n",
    "        }\n",
    "        \n",
    "        if(action != 2):\n",
    "            #pydirectinput.press(action_map[action]) # For Windows\n",
    "            pyautogui.press(action_map[action]) # For Non-Windows\n",
    "        \n",
    "        # Checks whether the game has ended\n",
    "        game_over, game_over_screen_capture = self.get_done()\n",
    "        \n",
    "        # Retrieves the new observation\n",
    "        new_observation = self.get_observation()\n",
    "        \n",
    "        # Rewards the AI model's agent\n",
    "        # As the game in this context is Dino Game, a reward is given for every frame the game has not reached\n",
    "        # the game over status, therefore the longer the Dino survives and the game runs, the greater the reward\n",
    "        reward = 1\n",
    "        \n",
    "        # Information Dictionary\n",
    "        # Required by stable baselines, even if unnecessary for the context and left empty\n",
    "        information_dictionary = {}\n",
    "        \n",
    "        # Return order very important and must not change\n",
    "        return new_observation, reward, game_over, information_dictionary\n",
    "    \n",
    "    # Visualizes the game\n",
    "    def render(self):\n",
    "        # Displays the pre-processed in game frame via screen capture\n",
    "        cv2.imshow('Game', np.array(self.screen_capture.grab(self.game_location))[:, :, :3])\n",
    "        \n",
    "        # Giving ourselves time for the image to actually render\n",
    "        # If the 'q' key is pressed on the keyboard then the frame will be closed\n",
    "        if(cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "           self.close()\n",
    "    \n",
    "    # Restarts the game\n",
    "    def reset(self):\n",
    "        \n",
    "        # 'Dino Game' restarts after a game over with a simple click anywhere on the screen\n",
    "        \n",
    "        # For Windows\n",
    "        #pydirectinput.click(x = 150 , y = 150)\n",
    "        #pydirectinput.press('space')\n",
    "        \n",
    "         # For Non-Windows\n",
    "        pyautogui.click(x = 150 , y = 150)\n",
    "        pyautogui.press('space')\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    # Closes down the observation\n",
    "    def close(self):\n",
    "        \n",
    "        # Will close the render function's windows\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    # Retrieves the desired portion of the observation\n",
    "    def get_observation(self):\n",
    "        \n",
    "        # Computer Vision Pre-processing\n",
    "        # Grabs a raw initial screen capture of the game using mss, wrapping it into an array via numpy to\n",
    "        # obtain the pixel values, and finally choosing only the 1st 3 color channels\n",
    "        raw_screen_capture = np.array(self.screen_capture.grab(self.game_location))[:, :, :3].astype(np.uint8)\n",
    "        \n",
    "        # Gravyscaling\n",
    "        grayscaled_screen_capture = cv2.cvtColor(raw_screen_capture, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Resizing\n",
    "        resized_grayscaled_screen_capture = cv2.resize(grayscaled_screen_capture, (100, 83))\n",
    "        \n",
    "        # Reformts the shape of the array to have channels first, as this is how PyTorch and stable baselines\n",
    "        # needs said format to be in to properly function\n",
    "        preprocessed_screen_capture = np.reshape(resized_grayscaled_screen_capture, (1, 83, 100))\n",
    "        \n",
    "        return preprocessed_screen_capture\n",
    "    \n",
    "    # Retrieves the game over text using OCR\n",
    "    def get_done(self):\n",
    "        \n",
    "        # Retrieves the game over screen\n",
    "        game_over_screen_capture = np.array(self.screen_capture.grab(self.game_over_location))[:, :, :3].astype(np.uint8)\n",
    "        \n",
    "        # Valid 'Game Over' text\n",
    "        # Utilizes 'GAME' and 'GAHE' as sometimes it mixes up the two, but should still be able to recognize\n",
    "        # that the 'player' in this situation has lost and now must either quit or restart\n",
    "        game_over_strings = ['GAME', 'GAHE']\n",
    "        \n",
    "        game_over = False\n",
    "        \n",
    "        # Carries out the actual optical character recognition, trying to extract any text from the given\n",
    "        # observation space into a string, and then comparing said string to whether or not it is the word 'GAME'\n",
    "        # Could otherwise use image recognition if the 'Game Over' portion does not change\n",
    "        #result = pytesseract.image_to_string(game_over_screen_capture)[ : 4] # Google OCR not yet installed\n",
    "        \n",
    "        # Temporary Setup\n",
    "        result = 'GAME'\n",
    "        \n",
    "        # If True, then the training episode for the AI model will end\n",
    "        if(result in game_over_strings):\n",
    "            game_over = True\n",
    "        \n",
    "        return game_over, game_over_screen_capture\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_environment = WebGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering\n",
    "# Opens a new window displaying what the computer is looking at and playing\n",
    "# May have to check differences on Windows, Ubuntu and macOS due to how new windows are handled\n",
    "game_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting\n",
    "game_environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b637ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For checking and testing purposes\n",
    "\n",
    "# Observation Space\n",
    "\n",
    "# Screen capture of the entire game\n",
    "#plt.imshow(game_environment.observation_space.sample()[0])\n",
    "#game_environment.get_observation()\n",
    "#plt.imshow(game_environment.get_observation())\n",
    "#plt.imshow(game_environment.get_observation()[0])\n",
    "#plt.imshow(cv2.cvtColor(game_environment.get_observation()[0], cv2.COLOR_BGR2RGB))\n",
    "# game_environment.get_observation().shape\n",
    "\n",
    "# Screen capture of the game over text within the game\n",
    "#plt.imshow(game_environment.get_done())\n",
    "#np.array(game_environment.get_done()).shape\n",
    "\n",
    "# Action Space\n",
    "#game_environment.action_space.sample()\n",
    "#game_environment.observation_space.sample()\n",
    "\n",
    "# Game Over\n",
    "#game_over, game_over_screen_capture = game_environment.get_done()\n",
    "#plt.imshow(game_over)\n",
    "#print(game_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7620d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_environment = WebGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_observation = testing_environment.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f88228",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(testing_observation[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_over, game_over_screen_capture = testing_environment.get_done()\n",
    "plt.imshow(game_over)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What pytesseract reads from the 'GAME OVER' screen capture observation\n",
    "#pytesseract.image_to_string(came_over_screen_capture)\n",
    "#pytesseract.image_to_string(came_over_screen_capture)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c86c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plays 10 games whilst taking random actions\n",
    "for episode in range(10):\n",
    "    observation = game_environment.reset()\n",
    "    game_over = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while(not game_over):\n",
    "        observation, reward, game_over, information = game_environment.step(game_environment.action_space.sample())\n",
    "        total_reward += reward\n",
    "        \n",
    "    print(f'Total Reward: {total_reward} - Episode: {episode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ff07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks whether or not the environment is valid\n",
    "env_checker.check_env(game_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08653887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose = 1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok = True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIRECTORY = '../Models/Training/'\n",
    "LOG_DIRECTORY = '../Models/Logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model every 'x' number of steps / frames\n",
    "callback = TrainAndLoggingCallback(check_freq = 1000, save_path = CHECKPOINT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building & Training The Deep Q-Network (DQN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'buffer_size': Adjust accordignly to how much RAM is available upon the machine running the training\n",
    "# 'learning_start': Start learning after the first 1000 frames / steps\n",
    "dino_game_model = DQN('CnnPolicy', game_environment, tensorboard_log = LOG_DIRECTORY, \n",
    "                      verbose = 1, buffer_size = 1200000, learning_starts = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kicks off the model AI's training\n",
    "# 'total_timesteps': Equivalent to epochs in tensforflow, how low to train essentially\n",
    "# 88,000 steps takes all night\n",
    "dino_game_model.learn(total_timesteps = 5000, callback = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b706581",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_game_model = DQN.load('../Models/dinoGameModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plays 10 games utilizing the model's predictions\n",
    "for episode in range(10):\n",
    "    observation = game_environment.reset()\n",
    "    game_over = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while(not game_over):\n",
    "        action, _ = dino_game_model.predict(observation)\n",
    "        observation, reward, game_over, information = game_environment.step(int(action))\n",
    "        total_reward += reward\n",
    "        \n",
    "    print(f'Total Reward: {total_reward} - Episode: {episode}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
